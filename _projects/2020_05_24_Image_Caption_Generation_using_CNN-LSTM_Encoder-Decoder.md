---
title: "Image Caption Generation using CNN-LSTM Encoder-Decoder"
collection: projects
urlslug: "Image_Caption_Generation_using_CNN-LSTM_Encoder-Decoder"
type: "Personal"
permalink: /projects/2020_05_24_Image_Caption_Generation_using_CNN-LSTM_Encoder-Decoder
contributors: "Md Mesbahur Rahman"
contribution: "Pre-processed the images in the MS COCO Dataset using PyTorch Transforms and converted the captions in the training set into sequence of integers using BOW vocabulary dictionary with a vocabulary threshold of 5. Defined and trained a CNN encoder and a LSTM Decoder on top of a time distributed embedding layer by using pretrained RESNET50 model as a feature extractor to encode an input image into a fixed
embed sized vector and then used LSTM decoder to generate captions from the output embedding vector of the CNN encoder. Configurations of the data pre-processing and CNN encoder and LSTM decoder were inspired from [this paper](https://arxiv.org/pdf/1411.4555.pdf). Then inference was done on the 'test' portion of the MS COCO dataset."
date: 2020-05-24
teaserurl: 'image_captioning_infer2.png'
codeurl: 'https://github.com/mmrahman-utexas/Image_Caption_Generator'
excerpt: 'Image caption generation is a widely used application of sequential generative model. In this project, I designed and trained a CNN-LSTM encoder-decoder architecture for generating caption from an input image. I did this project as part of the requirement of gaduating "Computer Vision Nanodegree" from Udacity.'
---

Md Mesbahur Rahman

**Description:**
Image caption generation is a widely used application of sequential generative model. In this project, I designed and trained a CNN-LSTM encoder-decoder architecture for generating caption from an input image. I did this project as part of the requirement of gaduating "Computer Vision Nanodegree" from Udacity.

**My contribution:**
Pre-processed the images in the MS COCO Dataset using PyTorch Transforms and converted the captions in the training set into sequence of integers using BOW vocabulary dictionary with a vocabulary threshold of 5. Defined and trained a CNN encoder and a LSTM Decoder on top of a time distributed embedding layer by using pretrained RESNET50 model as a feature extractor to encode an input image into a fixed
embed sized vector and then used LSTM decoder to generate captions from the output embedding vector of the CNN encoder. Configurations of the data pre-processing and CNN encoder and LSTM decoder were inspired from [this paper](https://arxiv.org/pdf/1411.4555.pdf). Then inference was done on the 'test' portion of the MS COCO dataset.

**Resources:** [[Code](https://github.com/mmrahman-utexas/Image_Caption_Generator)]