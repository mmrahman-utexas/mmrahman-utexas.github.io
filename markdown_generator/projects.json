[
    {
        "title": "Designing and Training a Fully Attentive Multimodal Transformer Network for Medical Visual Question Answering Task",
        "contributors": "Md Mesbahur Rahman",
        "type": "Academic",
        "project_date": "2023-12-05", 
        "url_slug": "csml-ut-mscs-med-vqa-project", 
        "teaser_url": "medvqa-pred.png",
        "report_url": "https://mmrahman-utexas.github.io/files/mr58937_Rahman_Md_Mesbahur_final_report_csml_med_vqa.pdf",
        "code_url": "",
        "excerpt": "Medical Question Answering is a very important and impactful application of Multi-modal learning. It can contribute to the interpretability of machine learning model in medical applications, reduce workload of medical professional, and can be a part of fully automated healthcare system. In this project, we have done a background research on the state of the art of Medical Visual Question Answering research. Based on some latest well performing paper, we propose our own fully attention based Transformer only network for solving the medical visual question answering task by treating a multi-class classification problem. We also present some analysis on hyperparameter tuning of the model, compare its performance with models from some other notable papers and suggest some future improvements of our model.",
        "my_contribution": "Conducted background research on the state of the art of Medical Visual Question Answering research. Based on some latest well performing paper, proposed a novel fully attention-based Transformer only network for solving the medical visual question answering task by treating a multi-class classification problem. Trained the proposed model on the train set of VQA-RAD dataset and our model showed encouraging result on the test-set of the VQA dataset. Also presented some analysis on hyperparameter tuning of the model, compared its performance with models from some other notable papers and suggested some future improvements of our model including steps like pre-training the model on much larger medical vision language datasets."
    },
    {
        "title": "Analyzing and Mitigating Dataset Artifacts",
        "contributors": "Md Mesbahur Rahman",
        "type": "Academic",
        "project_date": "2022-12-11", 
        "url_slug": "nlp-ut-mscs-nlp-project", 
        "teaser_url": "sample-number-vs-f1.png",
        "report_url": "https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf",
        "code_url": "",
        "excerpt": "In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy
        measure neither conveys the whole story about a model's strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known
        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.",
        "my_contribution": "Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the model’s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the 'Adversarial SQuAD' dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result."
    },
    {
        "title": "Autonomous agents for realtime multiplayer ice-hockey",
        "contributors": "Md Mesbahur Rahman, Mohammad Aljubran, Nivethi Krithika, Shubham Bhardwaj",
        "type": "Academic",
        "project_date": "2020-12-14", 
        "url_slug": "ut-mscs-deep-learning-project", 
        "teaser_url": "puck_detection_regression.png",
        "report_url": "https://mmrahman-utexas.github.io/files/CS_395T___Deep_Learning__Final_Project_Manuscript.pdf",
        "code_url": "",
        "excerpt": "We designed an agent to
        play SuperTuxKart, and particularly compete with the AI oracle
        (and other classmate AI agents) in a 2v2 hockey game. Our
        strategy was to maximize puck possession and minimize puck
        distance to the opponent’s goal. Imitation Learning and DAgger
        could not perform sufficiently well when trained using the AI
        oracle of the game. Instead, an internal state controller was built
        and found superior to the AI, where it wins 70% of the time and
        scores an average of 3.1 goals per game when competing in 2v2
        against the AI oracle. Based on supervised learning, a planner
        was trained to detect puck presence and location. Playing 10 2v2
        games, this agent wins 30% of the games and scores an average
        of 1.2 goals per game. Future work can involve training a
        DAgger learner on the internal state controller.",
        "my_contribution": "Desihned, coded and trained multi-task fully convolutional CNN for vision stage of pipeline, wrote sections of report."
    },
    {
        "title": "Unsupervised Anomaly Detection Using Convolutional Autoencoder",
        "contributors": "Md Mesbahur Rahman",
        "type": "Academic",
        "project_date": "2020-05-31", 
        "url_slug": "Unsupervised-Anomaly-Detection-Using-Convolutional-Autoencoder", 
        "teaser_url": "anomaly-digit.png",
        "report_url": "",
        "code_url": "https://github.com/mmrahman-utexas/Unsupervised_Anomaly_Detection_Using_Convolutional_Autoencoder_Pytorch",
        "excerpt": "Anomaly detection is a very common and important problem to solve in industrial setting. There are several aproach exists for doing Anomaly Detection using Deep Learning. One of the most effective (both in terms of performace and model training cost) is to utilie unsupervized anomaly detection using Convolutional Autoencoder. In this project, I designed and trained an Convolutional Autoencoder model for detecting anomaly image (images of digit 3 in MNIST dataset) by considfering images of digit 1 as regular image.",
        "my_contribution": "Built an unsupervised dataset from a supervised labeled dataset of MNIST dataset byremoving its labels. Then defined a Convolutional Autoencoder network in PyTorch and trained it on the unsupervised dataset and allowed the network to learn to reconstruct the training images containing regular images with a small percentage of anomaly images. Then during inference, calculated a reconstruction error (MSE) threshold based on a given percent quantile and declared an input image as an anomaly for who's the output image reconstruction error is above the preset threshold."
    },
    {
        "title": "Image Caption Generation using CNN LSTM Encoder Decoder",
        "contributors": "Md Mesbahur Rahman",
        "type": "Academic",
        "project_date": "2020-05-24", 
        "url_slug": "Image-Caption-Generation-using-CNN-LSTM-Encoder-Decoder", 
        "teaser_url": "image-captioning-infer2.png",
        "report_url": "",
        "code_url": "https://github.com/mmrahman-utexas/Image_Caption_Generator",
        "excerpt": "Image caption generation is a widely used application of sequential generative model. In this project, I designed and trained a CNN-LSTM encoder-decoder architecture for generating caption from an input image. I did this project as part of the requirement of gaduating 'Computer Vision Nanodegree' from Udacity.",
        "my_contribution": "Pre-processed the images in the MS COCO Dataset using PyTorch Transforms and converted the captions in the training set into sequence of integers using BOW vocabulary dictionary with a vocabulary threshold of 5. Defined and trained a CNN encoder and a LSTM Decoder on top of a time distributed embedding layer by using pretrained RESNET50 model as a feature extractor to encode an input image into a fixed
        embed sized vector and then used LSTM decoder to generate captions from the output embedding vector of the CNN encoder. Configurations of the data pre-processing and CNN encoder and LSTM decoder were inspired from [this paper](https://arxiv.org/pdf/1411.4555.pdf). Then inference was done on the 'test' portion of the MS COCO dataset."
    },
    {
        "title": "Facial Keypoint Detection using CNN Haar Cascade Classifier",
        "contributors": "Md Mesbahur Rahman",
        "type": "Academic",
        "project_date": "2017-09-08", 
        "url_slug": "Facial-Keypoint-Detection-using-CNN-Haar-Cascade-Classifier", 
        "teaser_url": "facial_leypoint_infer.png",
        "report_url": "",
        "code_url": "https://github.com/mmrahman-utexas/Facial_Keypoint_Detection_CNN_Regression_OpenCV_HaarCascade_PyTorch",
        "excerpt": "Facial keypoint detection is an important example of a computer vision problem which can be solved effectively by treating the problem as an image regression task and and trainign a CNN network for predicting the image location of the key-points. In this project, I trained a CNN network to predict important facial keypoints given an image of a human face.  I did this project as a requirements of graduating from Udacity's Computer Vision Nanodegree program.",
        "my_contribution": "Defined and trained a CNN on facial keypoint dataset from YouTube Faces Dataset using custom transformation in PyTorch to perform regression task to predict the location of 68 facial keypoints as inspired from [this paper](https://arxiv.org/pdf/1710.00977.pdf). During inference detected all the faces in an image using OpenCV's pre-trained Haar Cascade classifiers and predicted the location of 68 facial keypoints on those detected faces using our trained CNN network."
    }
]