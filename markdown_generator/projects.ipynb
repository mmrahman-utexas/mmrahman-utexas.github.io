{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Projects markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of projects with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). \n",
    "\n",
    "TODO: Make this work with BibTex and other databases, rather than Stuart's non-standard TSV format and citation style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: title, type, url_slug, venue, date, location, slides_url, video_url, excerpt, with a header at the top. Many of these fields can be blank, but the columns must be in the TSV.\n",
    "\n",
    "- Fields that cannot be blank: `title`, `url_slug`, `date`. All else can be blank. `type` defaults to \"Talk\" \n",
    "- `date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. \n",
    "    - The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/talks/YYYY-MM-DD-[url_slug]`\n",
    "    - The combination of `url_slug` and `date` must be unique, as it will be the basis for your filenames\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Designing and Training a Fully Attentive Multimodal Transformer Network for Medical Visual Question Answering Task\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2023-12-05\", \n",
      "        \"url_slug\": \"csml-ut-mscs-med-vqa-project\", \n",
      "        \"teaser_url\": \"medvqa-pred.png\",\n",
      "        \"report_url\": \"https://mmrahman-utexas.github.io/files/mr58937_Rahman_Md_Mesbahur_final_report_csml_med_vqa.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"Medical Question Answering is a very important and impactful application of Multi-modal learning. It can contribute to the interpretability of machine learning model in medical applications, reduce workload of medical professional, and can be a part of fully automated healthcare system. In this project, we have done a background research on the state of the art of Medical Visual Question Answering research. Based on some latest well performing paper, we propose our own fully attention based Transformer only network for solving the medical visual question answering task by treating a multi-class classification problem. We also present some analysis on hyperparameter tuning of the model, compare its performance with models from some other notable papers and suggest some future improvements of our model.\",\n",
      "        \"my_contribution\": \"Conducted background research on the state of the art of Medical Visual Question Answering research. Based on some latest well performing paper, proposed a novel fully attention-based Transformer only network for solving the medical visual question answering task by treating a multi-class classification problem. Trained the proposed model on the train set of VQA-RAD dataset and our model showed encouraging result on the test-set of the VQA dataset. Also presented some analysis on hyperparameter tuning of the model, compared its performance with models from some other notable papers and suggested some future improvements of our model including steps like pre-training the model on much larger medical vision language datasets.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Analyzing and Mitigating Dataset Artifacts\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2022-12-11\", \n",
      "        \"url_slug\": \"nlp-ut-mscs-nlp-project\", \n",
      "        \"teaser_url\": \"sample-number-vs-f1.png\",\n",
      "        \"report_url\": \"https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model's strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.\",\n",
      "        \"my_contribution\": \"Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the model’s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the 'Adversarial SQuAD' dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Autonomous agents for realtime multiplayer ice-hockey\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman, Mohammad Aljubran, Nivethi Krithika, Shubham Bhardwaj\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-12-14\", \n",
      "        \"url_slug\": \"ut-mscs-deep-learning-project\", \n",
      "        \"teaser_url\": \"puck_detection_regression.png\",\n",
      "        \"report_url\": \"https://mmrahman-utexas.github.io/files/CS_395T___Deep_Learning__Final_Project_Manuscript.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"We designed an agent to\n",
      "        play SuperTuxKart, and particularly compete with the AI oracle\n",
      "        (and other classmate AI agents) in a 2v2 hockey game. Our\n",
      "        strategy was to maximize puck possession and minimize puck\n",
      "        distance to the opponent’s goal. Imitation Learning and DAgger\n",
      "        could not perform sufficiently well when trained using the AI\n",
      "        oracle of the game. Instead, an internal state controller was built\n",
      "        and found superior to the AI, where it wins 70% of the time and\n",
      "        scores an average of 3.1 goals per game when competing in 2v2\n",
      "        against the AI oracle. Based on supervised learning, a planner\n",
      "        was trained to detect puck presence and location. Playing 10 2v2\n",
      "        games, this agent wins 30% of the games and scores an average\n",
      "        of 1.2 goals per game. Future work can involve training a\n",
      "        DAgger learner on the internal state controller.\",\n",
      "        \"my_contribution\": \"Desihned, coded and trained multi-task fully convolutional CNN for vision stage of pipeline, wrote sections of report.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Unsupervised Anomaly Detection Using Convolutional Autoencoder\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-05-31\", \n",
      "        \"url_slug\": \"Unsupervised-Anomaly-Detection-Using-Convolutional-Autoencoder\", \n",
      "        \"teaser_url\": \"anomaly-digit.png\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/mmrahman-utexas/Unsupervised_Anomaly_Detection_Using_Convolutional_Autoencoder_Pytorch\",\n",
      "        \"excerpt\": \"Anomaly detection is a very common and important problem to solve in industrial setting. There are several aproach exists for doing Anomaly Detection using Deep Learning. One of the most effective (both in terms of performace and model training cost) is to utilie unsupervized anomaly detection using Convolutional Autoencoder. In this project, I designed and trained an Convolutional Autoencoder model for detecting anomaly image (images of digit 3 in MNIST dataset) by considfering images of digit 1 as regular image.\",\n",
      "        \"my_contribution\": \"Built an unsupervised dataset from a supervised labeled dataset of MNIST dataset byremoving its labels. Then defined a Convolutional Autoencoder network in PyTorch and trained it on the unsupervised dataset and allowed the network to learn to reconstruct the training images containing regular images with a small percentage of anomaly images. Then during inference, calculated a reconstruction error (MSE) threshold based on a given percent quantile and declared an input image as an anomaly for who's the output image reconstruction error is above the preset threshold.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Image Caption Generation using CNN LSTM Encoder Decoder\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-05-24\", \n",
      "        \"url_slug\": \"Image-Caption-Generation-using-CNN-LSTM-Encoder-Decoder\", \n",
      "        \"teaser_url\": \"image-captioning-infer2.png\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/mmrahman-utexas/Image_Caption_Generator\",\n",
      "        \"excerpt\": \"Image caption generation is a widely used application of sequential generative model. In this project, I designed and trained a CNN-LSTM encoder-decoder architecture for generating caption from an input image. I did this project as part of the requirement of gaduating 'Computer Vision Nanodegree' from Udacity.\",\n",
      "        \"my_contribution\": \"Pre-processed the images in the MS COCO Dataset using PyTorch Transforms and converted the captions in the training set into sequence of integers using BOW vocabulary dictionary with a vocabulary threshold of 5. Defined and trained a CNN encoder and a LSTM Decoder on top of a time distributed embedding layer by using pretrained RESNET50 model as a feature extractor to encode an input image into a fixed\n",
      "        embed sized vector and then used LSTM decoder to generate captions from the output embedding vector of the CNN encoder. Configurations of the data pre-processing and CNN encoder and LSTM decoder were inspired from [this paper](https://arxiv.org/pdf/1411.4555.pdf). Then inference was done on the 'test' portion of the MS COCO dataset.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Facial Keypoint Detection using CNN Haar Cascade Classifier\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2017-09-08\", \n",
      "        \"url_slug\": \"Facial-Keypoint-Detection-using-CNN-Haar-Cascade-Classifier\", \n",
      "        \"teaser_url\": \"facial_leypoint_infer.png\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/mmrahman-utexas/Facial_Keypoint_Detection_CNN_Regression_OpenCV_HaarCascade_PyTorch\",\n",
      "        \"excerpt\": \"Facial keypoint detection is an important example of a computer vision problem which can be solved effectively by treating the problem as an image regression task and and trainign a CNN network for predicting the image location of the key-points. In this project, I trained a CNN network to predict important facial keypoints given an image of a human face.  I did this project as a requirements of graduating from Udacity's Computer Vision Nanodegree program.\",\n",
      "        \"my_contribution\": \"Defined and trained a CNN on facial keypoint dataset from YouTube Faces Dataset using custom transformation in PyTorch to perform regression task to predict the location of 68 facial keypoints as inspired from [this paper](https://arxiv.org/pdf/1710.00977.pdf). During inference detected all the faces in an image using OpenCV's pre-trained Haar Cascade classifiers and predicted the location of 68 facial keypoints on those detected faces using our trained CNN network.\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat projects.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contributors</th>\n",
       "      <th>type</th>\n",
       "      <th>project_date</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>teaser_url</th>\n",
       "      <th>report_url</th>\n",
       "      <th>code_url</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>my_contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Designing and Training a Fully Attentive Multi...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>csml-ut-mscs-med-vqa-project</td>\n",
       "      <td>medvqa-pred.png</td>\n",
       "      <td>https://mmrahman-utexas.github.io/files/mr5893...</td>\n",
       "      <td></td>\n",
       "      <td>Medical Question Answering is a very important...</td>\n",
       "      <td>Conducted background research on the state of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyzing and Mitigating Dataset Artifacts</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>nlp-ut-mscs-nlp-project</td>\n",
       "      <td>sample-number-vs-f1.png</td>\n",
       "      <td>https://mmrahman-utexas.github.io/files/UTCS_N...</td>\n",
       "      <td></td>\n",
       "      <td>In NLP research arena Benchmark datasets are o...</td>\n",
       "      <td>Trained ELECTRA-small model  on the SQuAD data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autonomous agents for realtime multiplayer ice...</td>\n",
       "      <td>Md Mesbahur Rahman, Mohammad Aljubran, Nivethi...</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>ut-mscs-deep-learning-project</td>\n",
       "      <td>puck_detection_regression.png</td>\n",
       "      <td>https://mmrahman-utexas.github.io/files/CS_395...</td>\n",
       "      <td></td>\n",
       "      <td>We designed an agent to\\n        play SuperTux...</td>\n",
       "      <td>Desihned, coded and trained multi-task fully c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unsupervised Anomaly Detection Using Convoluti...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>Unsupervised-Anomaly-Detection-Using-Convoluti...</td>\n",
       "      <td>anomaly-digit.png</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/mmrahman-utexas/Unsupervise...</td>\n",
       "      <td>Anomaly detection is a very common and importa...</td>\n",
       "      <td>Built an unsupervised dataset from a supervise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image Caption Generation using CNN LSTM Encode...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>Image-Caption-Generation-using-CNN-LSTM-Encode...</td>\n",
       "      <td>image-captioning-infer2.png</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/mmrahman-utexas/Image_Capti...</td>\n",
       "      <td>Image caption generation is a widely used appl...</td>\n",
       "      <td>Pre-processed the images in the MS COCO Datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Facial Keypoint Detection using CNN Haar Casca...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>Facial-Keypoint-Detection-using-CNN-Haar-Casca...</td>\n",
       "      <td>facial_leypoint_infer.png</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/mmrahman-utexas/Facial_Keyp...</td>\n",
       "      <td>Facial keypoint detection is an important exam...</td>\n",
       "      <td>Defined and trained a CNN on facial keypoint d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Designing and Training a Fully Attentive Multi...   \n",
       "1         Analyzing and Mitigating Dataset Artifacts   \n",
       "2  Autonomous agents for realtime multiplayer ice...   \n",
       "3  Unsupervised Anomaly Detection Using Convoluti...   \n",
       "4  Image Caption Generation using CNN LSTM Encode...   \n",
       "5  Facial Keypoint Detection using CNN Haar Casca...   \n",
       "\n",
       "                                        contributors      type project_date  \\\n",
       "0                                 Md Mesbahur Rahman  Academic   2023-12-05   \n",
       "1                                 Md Mesbahur Rahman  Academic   2022-12-11   \n",
       "2  Md Mesbahur Rahman, Mohammad Aljubran, Nivethi...  Academic   2020-12-14   \n",
       "3                                 Md Mesbahur Rahman  Academic   2020-05-31   \n",
       "4                                 Md Mesbahur Rahman  Academic   2020-05-24   \n",
       "5                                 Md Mesbahur Rahman  Academic   2017-09-08   \n",
       "\n",
       "                                            url_slug  \\\n",
       "0                       csml-ut-mscs-med-vqa-project   \n",
       "1                            nlp-ut-mscs-nlp-project   \n",
       "2                      ut-mscs-deep-learning-project   \n",
       "3  Unsupervised-Anomaly-Detection-Using-Convoluti...   \n",
       "4  Image-Caption-Generation-using-CNN-LSTM-Encode...   \n",
       "5  Facial-Keypoint-Detection-using-CNN-Haar-Casca...   \n",
       "\n",
       "                      teaser_url  \\\n",
       "0                medvqa-pred.png   \n",
       "1        sample-number-vs-f1.png   \n",
       "2  puck_detection_regression.png   \n",
       "3              anomaly-digit.png   \n",
       "4    image-captioning-infer2.png   \n",
       "5      facial_leypoint_infer.png   \n",
       "\n",
       "                                          report_url  \\\n",
       "0  https://mmrahman-utexas.github.io/files/mr5893...   \n",
       "1  https://mmrahman-utexas.github.io/files/UTCS_N...   \n",
       "2  https://mmrahman-utexas.github.io/files/CS_395...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "\n",
       "                                            code_url  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  https://github.com/mmrahman-utexas/Unsupervise...   \n",
       "4  https://github.com/mmrahman-utexas/Image_Capti...   \n",
       "5  https://github.com/mmrahman-utexas/Facial_Keyp...   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  Medical Question Answering is a very important...   \n",
       "1  In NLP research arena Benchmark datasets are o...   \n",
       "2  We designed an agent to\\n        play SuperTux...   \n",
       "3  Anomaly detection is a very common and importa...   \n",
       "4  Image caption generation is a widely used appl...   \n",
       "5  Facial keypoint detection is an important exam...   \n",
       "\n",
       "                                     my_contribution  \n",
       "0  Conducted background research on the state of ...  \n",
       "1  Trained ELECTRA-small model  on the SQuAD data...  \n",
       "2  Desihned, coded and trained multi-task fully c...  \n",
       "3  Built an unsupervised dataset from a supervise...  \n",
       "4  Pre-processed the images in the MS COCO Datase...  \n",
       "5  Defined and trained a CNN on facial keypoint d...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects = pd.read_json(\"projects.json\")\n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    if type(text) is str:\n",
    "        return \"\".join(html_escape_table.get(c,c) for c in text)\n",
    "    else:\n",
    "        return \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loc_dict = {}\n",
    "\n",
    "for row, item in projects.iterrows():\n",
    "    \n",
    "    md_filename = str(item.project_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.project_date) + \"-\" + item.url_slug \n",
    "    year = item.project_date[:4]\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    md += \"collection: projects\" + \"\\n\"\n",
    "    md += f'urlslug: \"{item.url_slug }\"\\n'\n",
    "    if len(str(item.type)) > 3:\n",
    "        md += 'type: \"' + item.type + '\"\\n'\n",
    "    else:\n",
    "        md += 'type: \"Project\"\\n'\n",
    "    md += \"permalink: /projects/\" + html_filename + \"\\n\"\n",
    "    if len(str(item.contributors)) > 3:\n",
    "        md += 'contributors: \"' + item.contributors + '\"\\n'\n",
    "    if len(str(item.my_contribution)) > 3:\n",
    "        md += 'contribution: \"' + item.my_contribution + '\"\\n'\n",
    "    md += \"date: \" + str(item.project_date) + \"\\n\"\n",
    "    if len(str(item.teaser_url)) > 5:\n",
    "        md += \"teaserurl: '\" + item.teaser_url + \"'\\n\"\n",
    "    report_url = None\n",
    "    if len(str(item.report_url)) > 5:\n",
    "        report_url = str(item.report_url)\n",
    "        md += \"reporturl: '\" + item.report_url + \"'\\n\"\n",
    "    code_url = None\n",
    "    if len(str(item.code_url)) > 5:\n",
    "        code_url = str(item.code_url)\n",
    "        md += \"codeurl: '\" + item.code_url + \"'\\n\"\n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"excerpt: '\" + html_escape(item.excerpt) + \"'\\n\"\n",
    "           \n",
    "    md += \"---\\n\"\n",
    "\n",
    "    more_info = []\n",
    "    if code_url is not None:\n",
    "        more_info.append(f'[[Code]({code_url})]')\n",
    "    if report_url is not None:\n",
    "        more_info.append(f'[[Technical report]({report_url})]')\n",
    "    if len(item.contributors) > 0:\n",
    "        md += f\"\\n{item.contributors}\\n\"\n",
    "    if len(str(item.excerpt)) > 4:\n",
    "        md += f\"\\n**Description:**\\n{html_escape(item.excerpt)}\\n\"\n",
    "    if len(str(item.my_contribution)) > 4:\n",
    "        md += f\"\\n**My contribution:**\\n{html_escape(item.my_contribution)}\\n\"\n",
    "    if len(more_info) > 0:\n",
    "        md += f\"\\n**Resources:** {' '.join(more_info)}\\n\"\n",
    "        \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "    #print(md)\n",
    "    \n",
    "    with open(\"../_projects/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the talks directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-08-Facial-Keypoint-Detection-using-CNN-Haar-Cascade-Classifier.md\n",
      "2020-05-24-Image-Caption-Generation-using-CNN-LSTM-Encoder-Decoder.md\n",
      "2020-05-31-Unsupervised-Anomaly-Detection-Using-Convolutional-Autoencoder.md\n",
      "2020-12-14-ut-mscs-deep-learning-project.md\n",
      "2022-12-11-nlp-ut-mscs-nlp-project.md\n",
      "2023-12-05-csml-ut-mscs-med-vqa-project.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Analyzing and Mitigating Dataset Artifacts\"\n",
      "collection: projects\n",
      "urlslug: \"nlp-ut-mscs-nlp-project\"\n",
      "type: \"Academic\"\n",
      "permalink: /projects/2022-12-11-nlp-ut-mscs-nlp-project\n",
      "contributors: \"Md Mesbahur Rahman\"\n",
      "contribution: \"Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the model’s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the 'Adversarial SQuAD' dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\"\n",
      "date: 2022-12-11\n",
      "teaserurl: 'sample-number-vs-f1.png'\n",
      "reporturl: 'https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf'\n",
      "excerpt: 'In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model&apos;s strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.'\n",
      "---\n",
      "\n",
      "Md Mesbahur Rahman\n",
      "\n",
      "**Description:**\n",
      "In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model&apos;s strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.\n",
      "\n",
      "**My contribution:**\n",
      "Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the model’s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the &apos;Adversarial SQuAD&apos; dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\n",
      "\n",
      "**Resources:** [[Technical report](https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf)]\n"
     ]
    }
   ],
   "source": [
    "!cat ../_projects/2022-12-11-nlp-ut-mscs-nlp-project.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ghpages')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "06f7e23954b94c5e9c6c2692449a9adafbf148265f2ef39a8350fb6de62c0ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
