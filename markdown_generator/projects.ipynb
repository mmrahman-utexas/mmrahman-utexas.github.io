{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Projects markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of projects with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). \n",
    "\n",
    "TODO: Make this work with BibTex and other databases, rather than Stuart's non-standard TSV format and citation style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: title, type, url_slug, venue, date, location, slides_url, video_url, excerpt, with a header at the top. Many of these fields can be blank, but the columns must be in the TSV.\n",
    "\n",
    "- Fields that cannot be blank: `title`, `url_slug`, `date`. All else can be blank. `type` defaults to \"Talk\" \n",
    "- `date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. \n",
    "    - The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/talks/YYYY-MM-DD-[url_slug]`\n",
    "    - The combination of `url_slug` and `date` must be unique, as it will be the basis for your filenames\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Analyzing and Mitigating Dataset Artifacts\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2022-12-11\", \n",
      "        \"url_slug\": \"nlp-ut-mscs-nlp-project\", \n",
      "        \"teaser_url\": \"sample-number-vs-f1.png\",\n",
      "        \"report_url\": \"https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model's strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.\",\n",
      "        \"my_contribution\": \"Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the modelâ€™s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the 'Adversarial SQuAD' dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Unsupervised Anomaly Detection Using Convolutional Autoencoder\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-05-31\", \n",
      "        \"url_slug\": \"Unsupervised-Anomaly-Detection-Using-Convolutional-Autoencoder\", \n",
      "        \"teaser_url\": \"anomaly-digit.png\",\n",
      "        \"report_url\": \"https://github.com/mmrahman-utexas/Unsupervised_Anomaly_Detection_Using_Convolutional_Autoencoder_Pytorch\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"Anomaly detection is a very common and important problem to solve in industrial setting. There are several aproach exists for doing Anomaly Detection using Deep Learning. One of the most effective (both in terms of performace and model training cost) is to utilie unsupervized anomaly detection using Convolutional Autoencoder. In this project, I designed and trained an Convolutional Autoencoder model for detecting anomaly image (images of digit 3 in MNIST dataset) by considfering images of digit 1 as regular image.\",\n",
      "        \"my_contribution\": \"Built an unsupervised dataset from a supervised labeled dataset of MNIST dataset byremoving its labels. Then defined a Convolutional Autoencoder network in PyTorch and trained it on the unsupervised dataset and allowed the network to learn to reconstruct the training images containing regular images with a small percentage of anomaly images. Then during inference, calculated a reconstruction error (MSE) threshold based on a given percent quantile and declared an input image as an anomaly for who's the output image reconstruction error is above the preset threshold.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Image Caption Generation using CNN LSTM Encoder Decoder\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-05-24\", \n",
      "        \"url_slug\": \"Image-Caption-Generation-using-CNN-LSTM-Encoder-Decoder\", \n",
      "        \"teaser_url\": \"image-captioning-infer2.png\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/mmrahman-utexas/Image_Caption_Generator\",\n",
      "        \"excerpt\": \"Image caption generation is a widely used application of sequential generative model. In this project, I designed and trained a CNN-LSTM encoder-decoder architecture for generating caption from an input image. I did this project as part of the requirement of gaduating 'Computer Vision Nanodegree' from Udacity.\",\n",
      "        \"my_contribution\": \"Pre-processed the images in the MS COCO Dataset using PyTorch Transforms and converted the captions in the training set into sequence of integers using BOW vocabulary dictionary with a vocabulary threshold of 5. Defined and trained a CNN encoder and a LSTM Decoder on top of a time distributed embedding layer by using pretrained RESNET50 model as a feature extractor to encode an input image into a fixed\n",
      "        embed sized vector and then used LSTM decoder to generate captions from the output embedding vector of the CNN encoder. Configurations of the data pre-processing and CNN encoder and LSTM decoder were inspired from [this paper](https://arxiv.org/pdf/1411.4555.pdf). Then inference was done on the 'test' portion of the MS COCO dataset.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Facial Keypoint Detection using CNN Haar Cascade Classifier\",\n",
      "        \"contributors\": \"Md Mesbahur Rahman\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2017-09-08\", \n",
      "        \"url_slug\": \"Facial-Keypoint-Detection-using-CNN-Haar-Cascade-Classifier\", \n",
      "        \"teaser_url\": \"facial_leypoint_infer.png\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/mmrahman-utexas/Facial_Keypoint_Detection_CNN_Regression_OpenCV_HaarCascade_PyTorch\",\n",
      "        \"excerpt\": \"Facial keypoint detection is an important example of a computer vision problem which can be solved effectively by treating the problem as an image regression task and and trainign a CNN network for predicting the image location of the key-points. In this project, I trained a CNN network to predict important facial keypoints given an image of a human face.  I did this project as a requirements of graduating from Udacity's Computer Vision Nanodegree program.\",\n",
      "        \"my_contribution\": \"Defined and trained a CNN on facial keypoint dataset from YouTube Faces Dataset using custom transformation in PyTorch to perform regression task to predict the location of 68 facial keypoints as inspired from [this paper](https://arxiv.org/pdf/1710.00977.pdf). During inference detected all the faces in an image using OpenCV's pre-trained Haar Cascade classifiers and predicted the location of 68 facial keypoints on those detected faces using our trained CNN network.\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat projects.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contributors</th>\n",
       "      <th>type</th>\n",
       "      <th>project_date</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>teaser_url</th>\n",
       "      <th>report_url</th>\n",
       "      <th>code_url</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>my_contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyzing and Mitigating Dataset Artifacts</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>nlp-ut-mscs-nlp-project</td>\n",
       "      <td>sample-number-vs-f1.png</td>\n",
       "      <td>https://mmrahman-utexas.github.io/files/UTCS_N...</td>\n",
       "      <td></td>\n",
       "      <td>In NLP research arena Benchmark datasets are o...</td>\n",
       "      <td>Trained ELECTRA-small model  on the SQuAD data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unsupervised Anomaly Detection Using Convoluti...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>Unsupervised-Anomaly-Detection-Using-Convoluti...</td>\n",
       "      <td>anomaly-digit.png</td>\n",
       "      <td>https://github.com/mmrahman-utexas/Unsupervise...</td>\n",
       "      <td></td>\n",
       "      <td>Anomaly detection is a very common and importa...</td>\n",
       "      <td>Built an unsupervised dataset from a supervise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image Caption Generation using CNN LSTM Encode...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>Image-Caption-Generation-using-CNN-LSTM-Encode...</td>\n",
       "      <td>image-captioning-infer2.png</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/mmrahman-utexas/Image_Capti...</td>\n",
       "      <td>Image caption generation is a widely used appl...</td>\n",
       "      <td>Pre-processed the images in the MS COCO Datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facial Keypoint Detection using CNN Haar Casca...</td>\n",
       "      <td>Md Mesbahur Rahman</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>Facial-Keypoint-Detection-using-CNN-Haar-Casca...</td>\n",
       "      <td>facial_leypoint_infer.png</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/mmrahman-utexas/Facial_Keyp...</td>\n",
       "      <td>Facial keypoint detection is an important exam...</td>\n",
       "      <td>Defined and trained a CNN on facial keypoint d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        contributors  \\\n",
       "0         Analyzing and Mitigating Dataset Artifacts  Md Mesbahur Rahman   \n",
       "1  Unsupervised Anomaly Detection Using Convoluti...  Md Mesbahur Rahman   \n",
       "2  Image Caption Generation using CNN LSTM Encode...  Md Mesbahur Rahman   \n",
       "3  Facial Keypoint Detection using CNN Haar Casca...  Md Mesbahur Rahman   \n",
       "\n",
       "       type project_date                                           url_slug  \\\n",
       "0  Academic   2022-12-11                            nlp-ut-mscs-nlp-project   \n",
       "1  Academic   2020-05-31  Unsupervised-Anomaly-Detection-Using-Convoluti...   \n",
       "2  Academic   2020-05-24  Image-Caption-Generation-using-CNN-LSTM-Encode...   \n",
       "3  Academic   2017-09-08  Facial-Keypoint-Detection-using-CNN-Haar-Casca...   \n",
       "\n",
       "                    teaser_url  \\\n",
       "0      sample-number-vs-f1.png   \n",
       "1            anomaly-digit.png   \n",
       "2  image-captioning-infer2.png   \n",
       "3    facial_leypoint_infer.png   \n",
       "\n",
       "                                          report_url  \\\n",
       "0  https://mmrahman-utexas.github.io/files/UTCS_N...   \n",
       "1  https://github.com/mmrahman-utexas/Unsupervise...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "\n",
       "                                            code_url  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  https://github.com/mmrahman-utexas/Image_Capti...   \n",
       "3  https://github.com/mmrahman-utexas/Facial_Keyp...   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  In NLP research arena Benchmark datasets are o...   \n",
       "1  Anomaly detection is a very common and importa...   \n",
       "2  Image caption generation is a widely used appl...   \n",
       "3  Facial keypoint detection is an important exam...   \n",
       "\n",
       "                                     my_contribution  \n",
       "0  Trained ELECTRA-small model  on the SQuAD data...  \n",
       "1  Built an unsupervised dataset from a supervise...  \n",
       "2  Pre-processed the images in the MS COCO Datase...  \n",
       "3  Defined and trained a CNN on facial keypoint d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects = pd.read_json(\"projects.json\")\n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    if type(text) is str:\n",
    "        return \"\".join(html_escape_table.get(c,c) for c in text)\n",
    "    else:\n",
    "        return \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loc_dict = {}\n",
    "\n",
    "for row, item in projects.iterrows():\n",
    "    \n",
    "    md_filename = str(item.project_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.project_date) + \"-\" + item.url_slug \n",
    "    year = item.project_date[:4]\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    md += \"collection: projects\" + \"\\n\"\n",
    "    md += f'urlslug: \"{item.url_slug }\"\\n'\n",
    "    if len(str(item.type)) > 3:\n",
    "        md += 'type: \"' + item.type + '\"\\n'\n",
    "    else:\n",
    "        md += 'type: \"Project\"\\n'\n",
    "    md += \"permalink: /projects/\" + html_filename + \"\\n\"\n",
    "    if len(str(item.contributors)) > 3:\n",
    "        md += 'contributors: \"' + item.contributors + '\"\\n'\n",
    "    if len(str(item.my_contribution)) > 3:\n",
    "        md += 'contribution: \"' + item.my_contribution + '\"\\n'\n",
    "    md += \"date: \" + str(item.project_date) + \"\\n\"\n",
    "    if len(str(item.teaser_url)) > 5:\n",
    "        md += \"teaserurl: '\" + item.teaser_url + \"'\\n\"\n",
    "    report_url = None\n",
    "    if len(str(item.report_url)) > 5:\n",
    "        report_url = str(item.report_url)\n",
    "        md += \"reporturl: '\" + item.report_url + \"'\\n\"\n",
    "    code_url = None\n",
    "    if len(str(item.code_url)) > 5:\n",
    "        code_url = str(item.code_url)\n",
    "        md += \"codeurl: '\" + item.code_url + \"'\\n\"\n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"excerpt: '\" + html_escape(item.excerpt) + \"'\\n\"\n",
    "           \n",
    "    md += \"---\\n\"\n",
    "\n",
    "    more_info = []\n",
    "    if code_url is not None:\n",
    "        more_info.append(f'[[Code]({code_url})]')\n",
    "    if report_url is not None:\n",
    "        more_info.append(f'[[Technical report]({report_url})]')\n",
    "    if len(item.contributors) > 0:\n",
    "        md += f\"\\n{item.contributors}\\n\"\n",
    "    if len(str(item.excerpt)) > 4:\n",
    "        md += f\"\\n**Description:**\\n{html_escape(item.excerpt)}\\n\"\n",
    "    if len(str(item.my_contribution)) > 4:\n",
    "        md += f\"\\n**My contribution:**\\n{html_escape(item.my_contribution)}\\n\"\n",
    "    if len(more_info) > 0:\n",
    "        md += f\"\\n**Resources:** {' '.join(more_info)}\\n\"\n",
    "        \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "    #print(md)\n",
    "    \n",
    "    with open(\"../_projects/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the talks directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-08-Facial-Keypoint-Detection-using-CNN-Haar-Cascade-Classifier.md\n",
      "2020-05-24-Image-Caption-Generation-using-CNN-LSTM-Encoder-Decoder.md\n",
      "2020-05-31-Unsupervised-Anomaly-Detection-Using-Convolutional-Autoencoder.md\n",
      "2022-12-11-nlp-ut-mscs-nlp-project.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Analyzing and Mitigating Dataset Artifacts\"\n",
      "collection: projects\n",
      "urlslug: \"nlp-ut-mscs-nlp-project\"\n",
      "type: \"Academic\"\n",
      "permalink: /projects/2022-12-11-nlp-ut-mscs-nlp-project\n",
      "contributors: \"Md Mesbahur Rahman\"\n",
      "contribution: \"Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the modelâ€™s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the 'Adversarial SQuAD' dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\"\n",
      "date: 2022-12-11\n",
      "teaserurl: 'sample-number-vs-f1.png'\n",
      "reporturl: 'https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf'\n",
      "excerpt: 'In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model&apos;s strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.'\n",
      "---\n",
      "\n",
      "Md Mesbahur Rahman\n",
      "\n",
      "**Description:**\n",
      "In NLP research arena Benchmark datasets are often used to compare the performance of different SOTA models. But a high held-out accuracy\n",
      "        measure neither conveys the whole story about a model&apos;s strengths and weaknesses nor it can guarantee that the model has meaningfully solved the dataset. The model can just learn some spurious correlation in the dataset and can still achieve some high accuracy. This phenomenon is known\n",
      "        as Dataset Artifacts and in this project, we tried to identify some cases of it for the ELECTRA-small (Clark et al., 2020) model on the SQuAD problem setting using Checklist and Adverserial Dataset frameworks and took attempt of mitigating some of the Dataset Artifacts using Dataset Inoculation by fine-tuning strategy.\n",
      "\n",
      "**My contribution:**\n",
      "Trained ELECTRA-small model  on the SQuAD dataset. Then we generated predictions for the respective dataset of Checklist sets and Adversarial SQuAD from this model using our own scripts. Then we used Checklist and Adversarial framework to identify some of the artifacts in the modelâ€™s learning. Implemented Inoculation by fine-tuning mwthod for mitigating dataset artifacts by taking our original Electra-small model training on the training set of the SQuAD dataset and fine-tuning it on a small subset sampled from the training set of the &apos;Adversarial SQuAD&apos; dataset. Then we evaluated dataset artifacts of this finetuned model using Checklist sets and Adversarial SQuAD and caompared with the original result.\n",
      "\n",
      "**Resources:** [[Technical report](https://mmrahman-utexas.github.io/files/UTCS_NLP_Analyzing_and_Mitigating_Dataset_Artifacts.pdf)]\n"
     ]
    }
   ],
   "source": [
    "!cat ../_projects/2022-12-11-nlp-ut-mscs-nlp-project.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ghpages')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "06f7e23954b94c5e9c6c2692449a9adafbf148265f2ef39a8350fb6de62c0ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
